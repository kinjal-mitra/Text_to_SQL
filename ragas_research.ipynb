{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e043f35",
   "metadata": {},
   "source": [
    "# RAGAs(Retrieval-Augmented Generation Assessment) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d249a",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2b0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Langchain and SQL Imports\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# RAGAS Evaluation Framework\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    AspectCritic,\n",
    "    RubricsScore,\n",
    "    ContextPrecision,\n",
    "    Faithfulness\n",
    ")\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25ad43",
   "metadata": {},
   "source": [
    "### LLM and Database Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8905c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database and LLM setup complete!\n"
     ]
    }
   ],
   "source": [
    "# LLM Setup (Google Gemini 2.0 Flash)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash',\n",
    "    api_key=os.environ.get(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Database Connection (same as app.py)\n",
    "db_password = os.environ.get(\"DB_PASSWORD\")\n",
    "host = 'localhost'\n",
    "port = '3306'\n",
    "username = 'root'\n",
    "database_schema = os.environ.get(\"DB_SCHEMA\")\n",
    "mysql_uri = f\"mysql+mysqlconnector://{username}:{db_password}@{host}:{port}/{database_schema}\"\n",
    "db = SQLDatabase.from_uri(mysql_uri, sample_rows_in_table_info=2)\n",
    "\n",
    "# LLM Setup for SQL Generation (same as app.py)\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "Remember : Only provide me the sql query do not include anything else. Provide me sql query in a single line do not add line breaks.\n",
    "Table Schema: {schema}\n",
    "Question: {question}\n",
    "SQL Query:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def get_schema_from_db(database: SQLDatabase) -> str:\n",
    "    \"\"\"Get the schema from the database.\"\"\"\n",
    "    table_info = database.get_table_info()\n",
    "    schema = \"\\n\".join(table_info)\n",
    "    return schema\n",
    "\n",
    "sql_chain = (\n",
    "    RunnablePassthrough.assign(schema=lambda _: get_schema_from_db(db))\n",
    "    | prompt\n",
    "    | llm.bind(stop=[\"\\nSQLResult:\"])\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def get_sql_query_from_question(question: str) -> str:\n",
    "    \"\"\"Get the SQL query from the natural language question.\"\"\"\n",
    "    sql_query = sql_chain.invoke({\"question\": question})\n",
    "    query = re.search(r\"```sql\\s*(.*?)\\s*```\", sql_query, re.DOTALL | re.IGNORECASE)\n",
    "    if query:\n",
    "        sql_query = query.group(1).strip()\n",
    "    return sql_query\n",
    "\n",
    "def execute_sql_query(sql_query: str):\n",
    "    \"\"\"Execute the SQL query and return the results.\"\"\"\n",
    "    try:\n",
    "        results = db.run(sql_query)\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"Database and LLM setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5026e0",
   "metadata": {},
   "source": [
    "### Prepare Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ce3a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data loaded!\n",
      "Number of test questions: 5\n"
     ]
    }
   ],
   "source": [
    "# Create your test cases\n",
    "test_data = {\n",
    "    'question': [\n",
    "        \"What was the budget of Product 12?\",\n",
    "        \"What are the names of top 5 selling products?\",\n",
    "        \"List 3 customer names and order quantity with the lowest Order Quantity.\",\n",
    "        \"Find the total number of states\",\n",
    "        \"What is the name of the customer with Customer Index = 1\"\n",
    "    ],\n",
    "    'ground_truth_sql': [\n",
    "        \"SELECT `2017 Budgets` FROM `2017_budgets` WHERE `Product Name` = 'Product 12'\",\n",
    "        \"SELECT T2.`Product Name` FROM `sales_order` AS T1 INNER JOIN products AS T2 ON T1.`Product Description Index` = T2.`Index` GROUP BY T2.`Product Name` ORDER BY SUM(T1.`Line Total`) DESC LIMIT 5;\",\n",
    "        \"SELECT T1.`Customer Names`, SUM(T2.`Order Quantity`) AS TotalQuantity FROM customers AS T1 JOIN sales_order AS T2 ON T1.`Customer Index` = T2.`Customer Name Index` GROUP BY T1.`Customer Names` ORDER BY TotalQuantity ASC LIMIT 3\"\n",
    "        \"SELECT count(DISTINCT `State`) FROM state_regions\",\n",
    "        \"SELECT `Customer Names` FROM customers WHERE `Customer Index` = 1\"\n",
    "    ],\n",
    "    'ground_truth_answer': [\n",
    "        \"1356976.996\",\n",
    "        [('Product 26',), ('Product 25',), ('Product 13',), ('Product 14',), ('Product 5',)],\n",
    "        [('Amerisourc Corp', '2022'), ('Mycone Ltd', '2208'), ('Voonyx Group', '2226')],\n",
    "        \"48\",\n",
    "        \"Geiss Company\"\n",
    "    ],\n",
    "    'contexts': [  # Database schema information\n",
    "        [\"Table: `2017_budgets` (`Product Name`, `2017 Budgets`)\"],\n",
    "        [\"Table: `sales_order` (`Line Total`, `Product Description Index`)\", \"Table: `products` (`Index`, `Product Name`)\"],\n",
    "        [\"Table: `customers` (`Customer Index`, `Customer Names`)\", \"Table: `sales_order` (`Order Quantity`, `Customer Name Index`)\"],\n",
    "        [\"Table: `regions` (`name`, `state`)\"],\n",
    "        [\"Table: `customers` (`Customer Index`, `Customer Names`)\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Test data loaded!\")\n",
    "print(f\"Number of test questions: {len(test_data['question'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6a070",
   "metadata": {},
   "source": [
    "### Generate predictions for our test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec558f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "\n",
      "Processing question 1/5: What was the budget of Product 12?\n",
      "Generated SQL: SELECT `2017 Budgets` FROM `2017_budgets` WHERE `Product Name` = 'Product 12';\n",
      "Generated Answer: [(1356976.996,)]\n",
      "\n",
      "Processing question 2/5: What are the names of top 5 selling products?\n",
      "Generated SQL: SELECT p.`Product Name` FROM sales_order so JOIN products p ON so.`Product Description Index` = p.`Index` GROUP BY p.`Product Name` ORDER BY SUM(so.`Line Total`) DESC LIMIT 5\n",
      "Generated Answer: [('Product 26',), ('Product 25',), ('Product 13',), ('Product 14',), ('Product 5',)]\n",
      "\n",
      "Processing question 3/5: List 3 customer names and order quantity with the lowest Order Quantity.\n",
      "Generated SQL: SELECT T1.`Customer Names`, T2.`Order Quantity` FROM customers AS T1 JOIN sales_order AS T2 ON T1.`Customer Index` = T2.`Customer Name Index` ORDER BY T2.`Order Quantity` ASC LIMIT 3\n",
      "Generated Answer: [('Ascend Ltd', 5), ('Ascend Ltd', 5), ('Skidoo Company', 5)]\n",
      "\n",
      "Processing question 4/5: Find the total number of states\n",
      "Generated SQL: SELECT count(DISTINCT `State`) FROM state_regions\n",
      "Generated Answer: [(48,)]\n",
      "\n",
      "Processing question 5/5: What is the name of the customer with Customer Index = 1\n",
      "Generated SQL: SELECT `Customer Names` FROM customers WHERE `Customer Index` = 1\n",
      "Generated Answer: [('Geiss Company',)]\n",
      "\n",
      "Prediction generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Generate SQL queries and answers using our system\n",
    "generated_queries = []\n",
    "generated_answers = []\n",
    "\n",
    "print(\"Generating predictions...\\n\")\n",
    "\n",
    "for i, question in enumerate(test_data['question'], 1):\n",
    "    print(f\"Processing question {i}/{len(test_data['question'])}: {question}\")\n",
    "    \n",
    "    # Generate SQL query\n",
    "    sql_query = get_sql_query_from_question(question)\n",
    "    generated_queries.append(sql_query)\n",
    "    print(f\"Generated SQL: {sql_query}\")\n",
    "    \n",
    "    # Execute query and get results\n",
    "    result = execute_sql_query(sql_query)\n",
    "    generated_answers.append(str(result))\n",
    "    print(f\"Generated Answer: {result}\\n\")\n",
    "\n",
    "    time.sleep(10) # Pause to avoid rate limits\n",
    "\n",
    "print(\"Prediction generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6b0b1",
   "metadata": {},
   "source": [
    "### Prepare Dataset for RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92064309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS dataset created!\n",
      "\n",
      "Dataset structure:\n",
      "Dataset({\n",
      "    features: ['question', 'contexts', 'answer', 'ground_truth'],\n",
      "    num_rows: 5\n",
      "})\n",
      "\n",
      "First example:\n",
      "{'question': 'What was the budget of Product 12?', 'contexts': ['Table: `2017_budgets` (`Product Name`, `2017 Budgets`)'], 'answer': '[(1356976.996,)]', 'ground_truth': '1356976.996'}\n"
     ]
    }
   ],
   "source": [
    "# Prepare data in RAGAS format\n",
    "ragas_data = {\n",
    "    'question': test_data['question'],\n",
    "    'contexts': test_data['contexts'],\n",
    "    'answer': generated_answers,\n",
    "    'ground_truth': [str(gt) for gt in test_data['ground_truth_answer']]\n",
    "}\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "ragas_dataset = Dataset.from_dict(ragas_data)\n",
    "\n",
    "print(\"RAGAS dataset created!\")\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(ragas_dataset)\n",
    "print(f\"\\nFirst example:\")\n",
    "print(ragas_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe061b3",
   "metadata": {},
   "source": [
    "### Initialize Evaluator LLM and Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8758827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator LLM (Claude) initialized!\n",
      "Evaluator Embeddings model initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChatAnthropic for evaluation\n",
    "evaluator_llm = ChatAnthropic(\n",
    "    #model=\"claude-3-5-sonnet-20241022\",\n",
    "    model=\"claude-3-5-haiku-latest\",\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Evaluator LLM (Claude) initialized!\")\n",
    "\n",
    "\n",
    "# Initialize HuggingFace Embeddings model for evaluation\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Using a lighter model\n",
    "    cache_folder=\"./models\",  # Specify cache directory\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Ensure proper encoding\n",
    ")\n",
    "\n",
    "print(\"Evaluator Embeddings model initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb14cf7",
   "metadata": {},
   "source": [
    "### Run RAGAs Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929c9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "\n",
    "\n",
    "# create a RunConfig to reduce parallelism, increase timeouts & add retries\n",
    "run_config = RunConfig(\n",
    "    max_workers=1,   # limit parallel LLM requests\n",
    "    timeout=240,     # seconds per LLM call\n",
    "    max_retries=2,\n",
    "    max_wait=120,       # number of retry attempts on transient errors\n",
    "    log_tenacity=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3695655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAGAS evaluation...\n",
      "Metrics to evaluate: ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', 'answer_correctness', 'answer_similarity']\n",
      "\n",
      "Preparing to evaluate metric: faithfulness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e99232d947d485b99238d641398898c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Overall results for faithfulness:\n",
      "faithfulness             : 0.0000\n",
      "Preparing to evaluate metric: answer_relevancy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b07645b1a046a4840d51d0d13264b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[2]: RateLimitError(Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (5869db6b-d040-4b1e-a7da-994c139b3d65) of 5 requests per minute. For details, refer to: https://docs.claude.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}, 'request_id': 'req_011CUbyP8CoJK4wLjk9aY1tK'})\n",
      "Exception raised in Job[3]: RateLimitError(Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (5869db6b-d040-4b1e-a7da-994c139b3d65) of 5 requests per minute. For details, refer to: https://docs.claude.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}, 'request_id': 'req_011CUbySiAYd2iRZk4u7ek5x'})\n",
      "Exception raised in Job[4]: RateLimitError(Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'This request would exceed the rate limit for your organization (5869db6b-d040-4b1e-a7da-994c139b3d65) of 5 requests per minute. For details, refer to: https://docs.claude.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}, 'request_id': 'req_011CUbyWCzyMq2JpuLhxD7Xg'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Overall results for answer_relevancy:\n",
      "answer_relevancy         : 0.4976\n",
      "Preparing to evaluate metric: context_precision\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237f820c00824e02b7bdcb49a616bca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Overall results for context_precision:\n",
      "context_precision        : 0.2000\n",
      "Preparing to evaluate metric: context_recall\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2a474b05cd42289e79edb145500c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Overall results for context_recall:\n",
      "context_recall           : 0.2000\n",
      "Preparing to evaluate metric: answer_correctness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53155a5828c14801b52db5b55219a962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Overall results for answer_correctness:\n",
      "answer_correctness       : 0.7729\n",
      "Preparing to evaluate metric: answer_similarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "162ed0417c5c4ba48eb7038951de35d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Overall results for answer_similarity:\n",
      "answer_similarity        : 0.6918\n",
      "============================================================\n",
      "\n",
      " Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Define metrics to evaluate\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    "]\n",
    "\n",
    "print(\"Starting RAGAS evaluation...\")\n",
    "print(f\"Metrics to evaluate: {[m.name for m in metrics]}\\n\")\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f\"Preparing to evaluate metric: {metric.name}\")\n",
    "    # Run evaluation\n",
    "    results = evaluate(\n",
    "        dataset=ragas_dataset,\n",
    "        metrics=[metric],\n",
    "        llm=evaluator_llm,\n",
    "        embeddings=embeddings_model,\n",
    "        run_config=run_config,\n",
    "        raise_exceptions=False\n",
    "    )\n",
    "\n",
    "    results_df = results.to_pandas()\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nOverall results for {metric.name}:\")\n",
    "    score = results_df[metric.name].mean()\n",
    "    print(f\"{metric.name:25s}: {score:.4f}\")\n",
    "\n",
    "    time.sleep(300)  # Pause between evaluations\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n Evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
